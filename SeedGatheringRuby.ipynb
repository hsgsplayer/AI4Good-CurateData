{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805e9d4c-f390-4aba-9ec8-627b9ed85aea",
   "metadata": {},
   "source": [
    "### SEED GATHERING GET CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323fa80d-e5d6-4fb8-a101-fc10e48848e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tree_sitter_parser import LANGUAGE, make_parser, node_to_string\n",
    "import datasets\n",
    "import os\n",
    "import signal\n",
    "from multiprocessing import Pool\n",
    "import boto3\n",
    "import smart_open\n",
    "from datasets import load_dataset,Dataset\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "def download_contents(blob_id, src_encoding):\n",
    "    s3_url = f\"s3://softwareheritage/content/{blob_id}\"\n",
    "    with smart_open.open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n",
    "        content = fin.read().decode(src_encoding)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8ee925-5001-4a30-b29f-5741f8d173a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPLEVEL_DOCSTRING_QUERY = LANGUAGE.query(\"\"\"\n",
    "# (\n",
    "#     (function_definition\n",
    "#       name: (identifier)\n",
    "#       body: (block .\n",
    "#         (expression_statement\n",
    "#             (string\n",
    "#                 (string_start) @docstring.start\n",
    "#                 (string_content)\n",
    "#                 (string_end) @docstring.end)))) @function.def\n",
    "#     (#eq? @docstring.start \"\\\\\\\"\\\\\\\"\\\\\\\"\")\n",
    "#     (#eq? @docstring.end \"\\\\\\\"\\\\\\\"\\\\\\\"\")\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "TOPLEVEL_DOCSTRING_QUERY_RUBY = LANGUAGE.query(\"\"\"\n",
    "(\n",
    "  [\n",
    "    (method\n",
    "      name: (_) @name) @definition.method\n",
    "  ]\n",
    "  (#strip! name \"^#\\\\s*\")\n",
    "  (#select-adjacent! name @definition.method)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def get_fns_with_docstrings(src, tree):\n",
    "    captures = TOPLEVEL_DOCSTRING_QUERY_RUBY.captures(tree.root_node)\n",
    "    res = []\n",
    "    for capture in captures:\n",
    "        node, ty = capture\n",
    "        if ty != \"definition.method\":\n",
    "            continue\n",
    "        # if the starting col is not 0, then it's not a top-level fn\n",
    "        _, col = node.start_point\n",
    "        if col != 0:\n",
    "            continue\n",
    "        res.append(node_to_string(src, node))\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_ex(parser, ex):\n",
    "    #ex = ex[\"content\"]\n",
    "    ex = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "    try:\n",
    "        buf = bytes(ex, \"utf8\")\n",
    "        tree = parser.parse(buf)\n",
    "        return get_fns_with_docstrings(buf, tree)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# if one parser segfaults, we can just make a new one and other parsers will still be fine\n",
    "# WE LOVE TREE SITTER!\n",
    "PARSERS = None\n",
    "\n",
    "\n",
    "def process_chunk(idx_and_chunk):\n",
    "    assert PARSERS is not None\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_funs = set()\n",
    "    \n",
    "    for ex in chunk:\n",
    "        chunk_new_funs.update(parse_ex(parser, ex))\n",
    "        break\n",
    "    return chunk_new_funs\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    global PARSERS\n",
    "    ds = datasets.load_dataset(\n",
    "        args.dataset,\n",
    "        data_dir=args.data_dir,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    funs = set()\n",
    "    PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "    total_len = len(ds)\n",
    "    CHUNK_SIZE = 1000 * args.num_workers\n",
    "\n",
    "    print(f\"Total length: {total_len}\")\n",
    "    print(f\"Chunk size: {CHUNK_SIZE}\")\n",
    "\n",
    "    chunk = []\n",
    "    p = Pool(args.num_workers)\n",
    "    for i, ex in enumerate(ds):\n",
    "        if i % (total_len // 100) == 0:\n",
    "            print(f\"{i}/{total_len}\")\n",
    "        try:\n",
    "            chunk.append(ex)\n",
    "            if len(chunk) == CHUNK_SIZE or i == total_len - 1:\n",
    "                print(f\"Processing chunk {i // CHUNK_SIZE}\")\n",
    "                # divide the chunk into NUM_WORKERS chunks\n",
    "                subchunk_size = len(chunk) // args.num_workers\n",
    "                subchunks = [chunk[i:i + subchunk_size]\n",
    "                             for i in range(0, len(chunk), subchunk_size)]\n",
    "                new_funs_iter = p.imap(\n",
    "                    process_chunk, [(i, subchunk) for i, subchunk in enumerate(subchunks)])\n",
    "                print(new_funs_iter)\n",
    "                print(\"Getting new functions\")\n",
    "                len_before = len(funs)\n",
    "                while True:\n",
    "                    try:\n",
    "                        def timeout_handler(_, __):\n",
    "                            raise KeyboardInterrupt  # it's fineeeeeee\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(60)\n",
    "                        funs.update(next(new_funs_iter))\n",
    "                        signal.alarm(0)\n",
    "                    except KeyboardInterrupt:\n",
    "                        signal.alarm(0)\n",
    "                        print(\"Keyboard interrupt. Terminating pool\")\n",
    "                        p.terminate()\n",
    "                        p = Pool(args.num_workers)\n",
    "                        break\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                signal.alarm(0)\n",
    "\n",
    "                PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "\n",
    "                print(\n",
    "                    f\"Done processing chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions\")\n",
    "\n",
    "                chunk = []\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            chunk = []\n",
    "\n",
    "        if i == total_len - 1:\n",
    "            break\n",
    "\n",
    "    p.close()\n",
    "\n",
    "    new_ds_dict = {\n",
    "        \"content\": list(funs),\n",
    "        \"id\": list(range(len(funs)))\n",
    "    }\n",
    "\n",
    "    new_ds = datasets.Dataset.from_dict(new_ds_dict)\n",
    "    #new_ds.push_to_hub(args.push, private=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74accea3-de2a-4b38-bbf2-b0c7f2be7aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "NUMWORKERS = os.cpu_count()\n",
    "print(NUMWORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8798ed1-24c7-4694-a97a-a381ab122392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165de5011aac4dc48875ea4e4b152e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"bigcode/the-stack-v2-dedup\", \"Ruby\", cache_dir = f\"../thai/stack\", streaming=True, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82541f12-dd45-44f7-ad0f-928224086086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 128000\n"
     ]
    }
   ],
   "source": [
    "funs = set()\n",
    "PARSERS = [make_parser() for _ in range(NUMWORKERS)]\n",
    "CHUNK_SIZE = 1000 * NUMWORKERS\n",
    "\n",
    "print(f\"Chunk size: {CHUNK_SIZE}\")\n",
    "\n",
    "chunk = []\n",
    "p = Pool(NUMWORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f242aa-2972-4037-b5d9-acc1f1cc7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      "Getting new functions\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "Done processing chunk 0. Got 11 new functions\n"
     ]
    }
   ],
   "source": [
    "for i, ex in enumerate(iter(ds)):\n",
    "    try:\n",
    "        chunk.append(ex)\n",
    "        if len(chunk) == 1000:\n",
    "            print(f\"Processing chunk {i // CHUNK_SIZE}\")\n",
    "            # divide the chunk into NUM_WORKERS chunks\n",
    "            subchunk_size = len(chunk) // NUMWORKERS\n",
    "            subchunks = [chunk[i:i + subchunk_size]\n",
    "                         for i in range(0, len(chunk), subchunk_size)]\n",
    "            new_funs_iter = p.imap(process_chunk, [(j, subchunk) for j, subchunk in enumerate(subchunks)])\n",
    "            print(\"Getting new functions\")\n",
    "            len_before = len(funs)\n",
    "            while True:\n",
    "                try:\n",
    "                    def timeout_handler(_, __):\n",
    "                        raise KeyboardInterrupt  # it's fineeeeeee\n",
    "                    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                    signal.alarm(60)\n",
    "                    funs.update(next(new_funs_iter))\n",
    "                    signal.alarm(0)\n",
    "                except KeyboardInterrupt:\n",
    "                    signal.alarm(0)\n",
    "                    print(\"Keyboard interrupt. Terminating pool\")\n",
    "                    p.terminate()\n",
    "                    p = Pool(NUMWORKERS)\n",
    "                    break\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            signal.alarm(0)\n",
    "\n",
    "            PARSERS = [make_parser() for _ in range(NUMWORKERS)]\n",
    "\n",
    "            print(\n",
    "                f\"Done processing chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions\")\n",
    "\n",
    "            chunk = []\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        chunk = []\n",
    "\n",
    "    if i >= 1000:\n",
    "        break\n",
    "\n",
    "\n",
    "p.close()\n",
    "new_ds_dict = {\n",
    "    \"content\": list(funs),\n",
    "    \"id\": list(range(len(funs)))\n",
    "}\n",
    "\n",
    "new_ds = datasets.Dataset.from_dict(new_ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99d75d7-bac9-4f47-bb2c-a5773f46ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'id'],\n",
       "    num_rows: 11\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "720c8fa5-2fe1-495a-842a-f32eaca53cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def update()\n",
      "  sql = \"UPDATE films SET (title, price) = ($1, $2) WHERE id = $3\"\n",
      "  values = [@title, @price]\n",
      "  SqlRunner.run(sql, values)\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "for ex in new_ds:\n",
    "    print(ex['content'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f5f20b-4648-4ac7-8a9d-3a35efc708de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa60b0-bf47-4b30-93e2-2ee8a71958bf",
   "metadata": {},
   "source": [
    "### SEED GATHERING HIGH-QUALITY SUBSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb1fe7e6-04d4-49c6-b19f-3ebf3d7642e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import signal\n",
    "import hashlib\n",
    "import os\n",
    "import argparse\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "from tree_sitter_parser import LANGUAGE, global_parser\n",
    "\n",
    "# RETURN_QUERY = LANGUAGE.query(\"\"\"\n",
    "# (return) @return\n",
    "# \"\"\")\n",
    "\n",
    "# def does_have_return(src):\n",
    "#     tree = global_parser.parse(bytes(src, \"utf8\"))\n",
    "#     root = tree.root_node\n",
    "#     captures = RETURN_QUERY.captures(root)\n",
    "#     for node, _ in captures:\n",
    "#         # if it doesn't have an argument, it's not a return with a value\n",
    "#         if len(node.children) <= 1:  # includes \"return\" itself\n",
    "#             continue\n",
    "#         else:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# runs sorbet in the given directory for typecheck, returns stdout\n",
    "# then, it logs the number of errors for each file\n",
    "def run_typeprof(d):\n",
    "    try:\n",
    "        outs = subprocess.run(\n",
    "            ['bundle', 'exec', 'rake', './typeprof/test'],  # Command to run\n",
    "            #cwd=d,               # Current working directory\n",
    "            capture_output=True, # Capture stdout and stderr\n",
    "            timeout=120,         # Timeout after 120 seconds\n",
    "            text=True            # Capture output as text\n",
    "        ).stdout\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    filemap = {}\n",
    "    lines = outs.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) >= 2:\n",
    "                file = parts[0].split(\"/\")[-1]\n",
    "                if file not in filemap:\n",
    "                    filemap[file] = 0\n",
    "                if \"error:\" in line:\n",
    "                    filemap[file] += 1\n",
    "\n",
    "    return filemap\n",
    "\n",
    "def typecheck_batch(files: List[str]) -> Dict[str, str]:\n",
    "    # Create a temporary directory using the tempfile module\n",
    "    filemap: Dict[str, str] = {}\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        for contents in files:\n",
    "            hash_object = hashlib.sha1(bytes(contents, \"utf8\"))\n",
    "            hex_dig = hash_object.hexdigest()\n",
    "            filemap[hex_dig] = contents\n",
    "            name = os.path.join(tempdir, hex_dig + \".rb\")\n",
    "            with open(name, \"w\") as f:\n",
    "                f.write(contents)\n",
    "\n",
    "        # Run typeprof in the temporary directory\n",
    "        typecheck_map = run_typeprof(tempdir)\n",
    "        print(typecheck_map)\n",
    "\n",
    "        if typecheck_map is None:\n",
    "            return {}\n",
    "\n",
    "        for contents, errors in typecheck_map.items():\n",
    "            no_py = contents.replace(\".rb\", \"\")\n",
    "            if errors == 0:\n",
    "                continue\n",
    "            if no_py in filemap:\n",
    "                del filemap[no_py]\n",
    "\n",
    "        print(f\"Pass rate: {len(filemap)}/{len(files)}\")\n",
    "        return filemap\n",
    "\n",
    "def infer_imports(code: str) -> str:\n",
    "    import autoimport\n",
    "    try:\n",
    "        def handler(signum, frame):\n",
    "            raise Exception(\"Timeout\")\n",
    "        signal.signal(signal.SIGALRM, handler)\n",
    "        signal.alarm(10)\n",
    "        inferred = autoimport.fix_code(code)\n",
    "        signal.alarm(0)\n",
    "        return inferred\n",
    "    except Exception as e:\n",
    "        signal.alarm(0)\n",
    "        print(f\"Error while inferring imports: {e}\")\n",
    "        return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5889ead-f07b-460e-8798-37df9b0c8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Filtering to only functions with return statements\")\n",
    "# ds = ds.filter(lambda ex: does_have_return(\n",
    "#     ex[\"content\"]), num_proc=os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d780ab-b991-46e4-b542-8b8802734c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'id'],\n",
       "    num_rows: 11\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c85b0-c68a-4d74-a783-01570aa7c02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db3fd22-9743-45e8-b17a-2195a5119c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 3512.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'bundle'\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if args.infer_imports:\n",
    "#     print(\"Inferring imports for functions\")\n",
    "#     ds = ds.map(lambda ex: {\"content\": infer_imports(\n",
    "#         ex[\"content\"])}, num_proc=os.cpu_count())\n",
    "\n",
    "batch = []\n",
    "max_i = len(ds) - 1\n",
    "\n",
    "new_ds = {\n",
    "    \"content\": [],\n",
    "    \"sha1\": [],\n",
    "    \"id\": [],\n",
    "}\n",
    "\n",
    "e_id = 0\n",
    "\n",
    "for i, ex in enumerate(tqdm(ds, total=len(ds))):\n",
    "    try:\n",
    "        code = ex[\"content\"]\n",
    "\n",
    "        batch.append(code)\n",
    "\n",
    "        if len(batch) == 250 or i == max_i:\n",
    "            filemap = typecheck_batch(batch)\n",
    "            for sha1, contents in filemap.items():\n",
    "                new_ds[\"content\"].append(contents)\n",
    "                new_ds[\"sha1\"].append(sha1)\n",
    "                new_ds[\"id\"].append(e_id)\n",
    "                e_id += 1\n",
    "            batch = []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"There was an error: {e}\")\n",
    "        continue\n",
    "\n",
    "new_ds_hf = datasets.Dataset.from_dict(new_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c04146-d49d-4cfd-83f9-0aa270ea3ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d692b2-f8be-48d9-9e75-b07ee1c422f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32e7cfd7-af58-4f7e-b88a-616014b4f866",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnew_ds_hf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(new_ds_hf['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "418df388-25a5-460d-825b-c95ede49b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./datasets/seed2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7673ee98-d98e-4766-931c-2a921293fec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5be377a95140e3a432fa0fceba4ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e6a4e5-2734-4602-80c9-5391521b391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_file(\"./datasets/seed2/data-00000-of-00001.arrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c67802-a173-4df2-b661-234745f0bfdf",
   "metadata": {},
   "source": [
    "### SEED GATHERING FILTER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d563dea-f9f0-4a8a-a0d9-4b5e70b1fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c2794-79e2-4c46-91b5-0c752bdf6e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01086352-e230-4267-9b7b-feab665f681a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'benchmark_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtree_sitter_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m global_parser, LANGUAGE, does_have_return, make_parser\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbenchmark_data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'benchmark_data'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "from tree_sitter_parser import global_parser, LANGUAGE, does_have_return, make_parser\n",
    "import benchmark_data\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# from vllm import LLM, SamplingParams\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adfc94c-0964-4eaa-9e9c-7749bb4cf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_BLOCK_QUERY = LANGUAGE.query(\"\"\"\n",
    "(\n",
    "[\n",
    "    (begin_block) @definition.begin_block\n",
    "    (end_block) @definition.end_block\n",
    "]\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "def template_few_shot_ruby(code, answer, rationale):\n",
    "    doc, code = ruby_extract_docstring(code)\n",
    "    assert answer == \"No\" or answer == \"Yes\"\n",
    "    prompt = f\"\"\"<issue_start>username_0: I have a function in Ruby and I'd like someone to check my description of this function.\n",
    "I'm doing this so that I can write a good docstring for this function.\n",
    "\n",
    "Here is the code for the function:\n",
    "```rb\n",
    "{code}\n",
    "```\n",
    "\n",
    "Here is my description of this program:\n",
    "```\n",
    "{doc}\n",
    "```\n",
    "\n",
    "Do not attempt to execute the function or to judge its correctness.\n",
    "Answer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\n",
    "Also, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\n",
    "My answer is: {answer}\n",
    "\n",
    "{rationale}\n",
    "\n",
    "Upvotes: 200\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "FEW_SHOTS_RUBY = [\n",
    "    (\n",
    "        '''def greet_user(name)\n",
    "  # Greet the user with a friendly message\n",
    "  \"Hello, #{name}!\"\n",
    "end''',\n",
    "        \"Yes\",\n",
    "        \"The docstring accurately describes the function. The `greet_user` method simply returns a greeting string using the provided `name`.\"\n",
    "    ),\n",
    "    (\n",
    "        '''def calculate_area(radius)\n",
    "  # Calculate the circumference of a circle given its radius\n",
    "  Math::PI * radius**2\n",
    "end''',\n",
    "        \"No\",\n",
    "        \"The description states that it calculates the circumference, but the method actually calculates the area of the circle based on the radius.\"\n",
    "    ),\n",
    "    (\n",
    "        '''def reverse_string(str)\n",
    "  # Reverse the characters in a string\n",
    "  str.reverse\n",
    "end''',\n",
    "        \"Yes\",\n",
    "        \"The docstring accurately describes the function's behavior. It reverses the characters in the provided string using Ruby's `reverse` method.\"\n",
    "    ),\n",
    "    (\n",
    "        '''def calculate_total(price, tax_rate)\n",
    "  # Adds tax to the price to get the total amount\n",
    "  price + (price * tax_rate)\n",
    "end''',\n",
    "        \"Yes\",\n",
    "        \"The docstring is clear and accurately describes the method's purpose of calculating the total price by adding tax.\"\n",
    "    ),\n",
    "    (\n",
    "        '''def print_numbers(n)\n",
    "  # Print numbers from 1 to n, inclusive\n",
    "  (1..n).each { |num| puts num }\n",
    "end''',\n",
    "        \"Yes\",\n",
    "        \"The docstring provides an accurate description of the method's function, which is to print numbers from 1 to `n`.\"\n",
    "    ),\n",
    "    (\n",
    "        '''def process_data(data)\n",
    "  # Process the data by removing duplicates and sorting it\n",
    "  data.uniq.sort\n",
    "end''',\n",
    "        \"Yes\",\n",
    "        \"The docstring accurately describes what the `process_data` method does. It removes duplicates and sorts the `data` array.\"\n",
    "    ),\n",
    "    (\n",
    "        '''def find_max(arr)\n",
    "  # Find the maximum number in an array of integers\n",
    "  arr.max\n",
    "end''',\n",
    "        \"Yes\",\n",
    "        \"The docstring provides an accurate description of the method, which finds the maximum number in the provided array using Ruby's `max` method.\"\n",
    "    ),\n",
    "    (\n",
    "        '''def send_email(address, subject, body)\n",
    "  # Set up and send an email\n",
    "  # Warning: This function does not implement sending\n",
    "end''',\n",
    "        \"No\",\n",
    "        \"The description implies that the method sends an email, but there is no implementation for actually sending an email in this code. It only sets up the parameters.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def prompt_fmt_ruby(code): \n",
    "    doc, code = ruby_extract_docstring(code) \n",
    "    random.shuffle(FEW_SHOTS_RUBY) \n",
    "    buf = \"\" \n",
    "    for few in FEW_SHOTS_RUBY: \n",
    "        buf += template_few_shot_ruby(*few) \n",
    "    buf += f\"\"\"<issue_start>username_0: I have a function in Ruby and I'd like someone to check my description of this function. \n",
    "I'm doing this so that I can write a good docstring for this function.\n",
    "\n",
    "Here is the code for the function:\n",
    "```rb\n",
    "{code}\n",
    "```\n",
    "\n",
    "Here is my description of this program:\n",
    "```\n",
    "{doc}\n",
    "```\n",
    "Do not attempt to execute the function or to judge its correctness. \n",
    "Answer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function. \n",
    "Also, answer with \"No\" if the description does not match the function. \n",
    "Upvotes: 100<issue_comment>username_1: Sure, no problem. I will be able to help. My answer is:\"\"\" \n",
    "    return buf\n",
    "\n",
    "def auto_dtype():\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        return \"bfloat16\"\n",
    "    return \"auto\"\n",
    "\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    chunks = []\n",
    "    for i in range(0, len(lst), n):\n",
    "        chunk = []\n",
    "        for j in range(n):\n",
    "            if i + j < len(lst):\n",
    "                chunk.append(lst[i + j])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcfa4b2-d3bb-4560-9578-8ce15d4d68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0992371-9ad5-4ad9-8aaa-d74b25eb4da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 examples. Running pre-filtering...\n",
      "num strings from mbpp_docstrings: 120\n",
      "num strings from mbpp_solutions: 120\n",
      "num strings from human_eval_docstrings: 164\n",
      "num strings from human_eval_solutions: 161\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(dataset)} examples. Running pre-filtering...\")\n",
    "\n",
    "BAD_WORDS = [\"todo\", \"fixme\", \"bug\"]\n",
    "BAD_IMPORTS = [\"argparse\", \"os\", \"subprocess\", \"sys\", \"setuptools\",\n",
    "               \"distutils\", \"matplotlib\", \"seaborn\"]\n",
    "BAD_IMPORTS = [f\"import {b}\" for b in BAD_IMPORTS] + \\\n",
    "    [f\"from {b}\" for b in BAD_IMPORTS]\n",
    "BAD_SUBSTRINGS = BAD_WORDS + BAD_IMPORTS\n",
    "\n",
    "bench_filter = benchmark_data.filter_out()\n",
    "all_bench = bench_filter[\"human_eval_docstrings\"] + \\\n",
    "    bench_filter[\"human_eval_solutions\"] + \\\n",
    "    bench_filter[\"mbpp_docstrings\"] + \\\n",
    "    bench_filter[\"mbpp_solutions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af077da-feac-4358-8702-e3c06927f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    }
   ],
   "source": [
    "def pre_filtering(ex):\n",
    "    code = ex[\"content\"]\n",
    "    code_bytes = code.encode('utf-8')\n",
    "\n",
    "    # filter out bad substrings\n",
    "    lower = code.lower()\n",
    "    for word in BAD_SUBSTRINGS:\n",
    "        if word in lower:\n",
    "            return False\n",
    "\n",
    "    for b in all_bench:\n",
    "        if b in code:  # contaminated sample!\n",
    "            return False\n",
    "\n",
    "    # too many lines of code -- say 150\n",
    "    lines = code.split(\"\\n\")\n",
    "    if len(lines) > 150:\n",
    "        return False\n",
    "\n",
    "    # filter functions which don't have an argument\n",
    "    # 1. find first def statement in lines\n",
    "    # 2. check if contains ():\n",
    "    for line in lines:\n",
    "        if line.startswith(\"def \") and \"():\" in line:\n",
    "            return False\n",
    "\n",
    "    # filter out functions with no return statement\n",
    "    # parser = make_parser()\n",
    "    # if not does_have_return(code, parser=parser):\n",
    "    #     return False\n",
    "\n",
    "    # try:\n",
    "    #     tree = global_parser.parse(code_bytes)\n",
    "    #     for node in tree.root_node.children:\n",
    "    #         print(node)\n",
    "    #     # print(FN_BLOCK_QUERY.captures(tree.root_node))\n",
    "    #     # block, _ = FN_BLOCK_QUERY.captures(tree.root_node)\n",
    "\n",
    "    #     # get the docstring, filter if not a docstring\n",
    "    #     # exp = block.children[0]\n",
    "    #     # print(exp)\n",
    "    #     # if not exp.type == 'expression_statement' and not exp.children[0].type == 'string':\n",
    "    #     #     return False\n",
    "\n",
    "    #     # docstring = exp.children[0]\n",
    "    #     # docstring_text = docstring.text.decode('utf-8')\n",
    "    #     # if not docstring_text.startswith('\"\"\"') and not docstring_text.endswith('\"\"\"'):\n",
    "    #     #     return False\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error in filtering: {e}\")\n",
    "    #     return False\n",
    "\n",
    "    return True  # all good!\n",
    "\n",
    "\n",
    "threads = os.cpu_count() - 1  # type: ignore\n",
    "dataset = dataset.filter(pre_filtering, num_proc=threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff87dd56-0118-4c02-a46e-777191048250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'sha1', 'id'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fe5b8-dbd1-4d93-96ca-46186b36bbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d9bb5f5-d5d8-468f-923e-8e8c94b99e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92183a-d4d5-4c8a-b7e9-9ae7db0144ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caba169-b4ca-49f7-9912-b163e0a08deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = model.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188670a-403c-4e47-b230-22a3896fe9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Now running stage 3 filtering on {len(dataset)} examples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f87ecf24-2e5a-4201-ba6c-cfcae19a9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unindent(s):\n",
    "    lines = s.splitlines()\n",
    "    non_blank_lines = [line for line in lines if line.strip()]\n",
    "    min_indent = min(len(line) - len(line.lstrip())\n",
    "                     for line in non_blank_lines) if non_blank_lines else 0\n",
    "    unindented_lines = [line[min_indent:] if len(\n",
    "        line) >= min_indent else line for line in lines]\n",
    "    return '\\n'.join(unindented_lines)\n",
    "\n",
    "\n",
    "def ruby_extract_docstring(code): \n",
    "    # In Ruby, comments usually start with # for each line of the description \n",
    "    lines = code.splitlines() \n",
    "    doc_lines = [] \n",
    "    code_lines = [] \n",
    "    in_doc = False \n",
    "    for line in lines: \n",
    "        if line.strip().startswith(\"#\"): \n",
    "            in_doc = True \n",
    "            doc_lines.append(line.strip(\"# \").strip()) \n",
    "        else: \n",
    "            in_doc = False \n",
    "            code_lines.append(line) \n",
    "            doc = \"\\n\".join(doc_lines) \n",
    "            code = \"\\n\".join(code_lines) \n",
    "    return doc, code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c13f00-6712-413d-a039-5964f3e7c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot prompt has 1806 tokens\n",
      "[27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 624, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 11047, 15030, 61022, 340, 220, 4149, 486, 1893, 353, 10578, 334, 17, 198, 408, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 3989, 47866, 279, 74926, 315, 264, 12671, 2661, 1181, 10578, 198, 13874, 19324, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 624, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 624, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 15757, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 624, 5050, 4226, 374, 25, 2308, 271, 785, 4008, 5302, 429, 432, 47417, 279, 74926, 11, 714, 279, 1714, 3520, 47417, 279, 3082, 315, 279, 12671, 3118, 389, 279, 10578, 382, 2324, 37835, 25, 220, 17, 15, 15, 27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 624, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 11047, 10784, 44746, 11, 3742, 9246, 340, 220, 3349, 488, 320, 6555, 353, 3742, 9246, 340, 408, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 3989, 72111, 3742, 311, 279, 3349, 311, 633, 279, 2790, 3311, 198, 13874, 19324, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 624, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 624, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 15757, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 624, 5050, 4226, 374, 25, 7414, 271, 785, 4629, 917, 374, 2797, 323, 29257, 16555, 279, 1714, 594, 7428, 315, 37614, 279, 2790, 3349, 553, 7842, 3742, 382, 2324, 37835, 25, 220, 17, 15, 15, 27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 624, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 9931, 3904, 4199, 340, 220, 607, 32081, 198, 408, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 3989, 45695, 279, 5766, 304, 264, 914, 198, 13874, 19324, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 624, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 624, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 15757, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 624, 5050, 4226, 374, 25, 7414, 271, 785, 4629, 917, 29257, 16555, 279, 729, 594, 7709, 13, 1084, 17431, 288, 279, 5766, 304, 279, 3897, 914, 1667, 23726, 594, 1565, 25903, 63, 1714, 382, 2324, 37835, 25, 220, 17, 15, 15, 27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 624, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 3624, 9172, 15434, 11, 3832, 11, 2487, 340, 408, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 3989, 1649, 705, 323, 3624, 458, 2551, 198, 12087, 25, 1096, 729, 1558, 537, 4211, 11628, 198, 13874, 19324, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 624, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 624, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 15757, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 624, 5050, 4226, 374, 25, 2308, 271, 785, 4008, 23945, 429, 279, 1714, 21308, 458, 2551, 11, 714, 1052, 374, 902, 8129, 369, 3520, 11628, 458, 2551, 304, 419, 2038, 13, 1084, 1172, 7289, 705, 279, 5029, 382, 2324, 37835, 25, 220, 17, 15, 15, 27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 624, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 1477, 6345, 10939, 340, 220, 2890, 6678, 198, 408, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 3989, 9885, 279, 7192, 1372, 304, 458, 1334, 315, 25780, 198, 13874, 19324, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 624, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 624, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 15757, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 624, 5050, 4226, 374, 25, 7414, 271, 785, 4629, 917, 5707, 458, 13382, 4008, 315, 279, 1714, 11, 892, 13719, 279, 7192, 1372, 304, 279, 3897, 1334, 1667, 23726, 594, 1565, 2810, 63, 1714, 382, 2324, 37835, 25, 220, 17, 15, 15, 27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 624, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 1882, 1769, 2592, 340, 220, 821, 6307, 23740, 10614, 198, 408, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 3989, 7423, 279, 821, 553, 17592, 42328, 323, 28273, 432, 198, 13874, 19324, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 624, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 624, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 15757, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 624, 5050, 4226, 374, 25, 7414, 271, 785, 4629, 917, 29257, 16555, 1128, 279, 1565, 4630, 1769, 63, 1714, 1558, 13, 1084, 28160, 42328, 323, 20853, 279, 1565, 691, 63, 1334, 382, 2324, 37835, 25, 220, 17, 15, 15, 27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 624, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 1173, 32964, 1445, 340, 220, 320, 16, 496, 77, 568, 9547, 314, 760, 2413, 91, 9521, 1629, 456, 408, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 3989, 8994, 5109, 504, 220, 16, 311, 308, 11, 28308, 198, 13874, 19324, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 624, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 624, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 15757, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 624, 5050, 4226, 374, 25, 7414, 271, 785, 4629, 917, 5707, 458, 13382, 4008, 315, 279, 1714, 594, 729, 11, 892, 374, 311, 1173, 5109, 504, 220, 16, 311, 1565, 77, 62338, 2324, 37835, 25, 220, 17, 15, 15, 27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 624, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 40786, 3317, 3153, 340, 220, 330, 9707, 11, 11273, 606, 92, 24734, 408, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 3989, 38, 3744, 279, 1196, 448, 264, 11657, 1943, 198, 13874, 19324, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 624, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 624, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 15757, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 624, 5050, 4226, 374, 25, 7414, 271, 785, 4629, 917, 29257, 16555, 279, 729, 13, 576, 1565, 70, 3744, 3317, 63, 1714, 4936, 4675, 264, 42113, 914, 1667, 279, 3897, 1565, 606, 62338, 2324, 37835, 25, 220, 17, 15, 15, 27, 11159, 4906, 29, 5113, 62, 15, 25, 358, 614, 264, 729, 304, 23726, 323, 358, 4172, 1075, 4325, 311, 1779, 847, 4008, 315, 419, 729, 13, 715, 40, 2776, 3730, 419, 773, 429, 358, 646, 3270, 264, 1661, 4629, 917, 369, 419, 729, 382, 8420, 374, 279, 2038, 369, 279, 729, 510, 73594, 10681, 198, 750, 17292, 4555, 715, 262, 3190, 262, 3190, 1494, 198, 13874, 19324, 8420, 374, 847, 4008, 315, 419, 2025, 510, 13874, 19324, 13874, 3989, 5404, 537, 4774, 311, 9026, 279, 729, 476, 311, 11651, 1181, 57323, 13, 715, 16141, 448, 330, 9454, 1, 476, 330, 2753, 1, 11649, 389, 421, 847, 4008, 702, 3322, 1995, 7484, 311, 312, 36925, 2764, 279, 729, 13, 715, 13394, 11, 4226, 448, 330, 2753, 1, 421, 279, 4008, 1558, 537, 2432, 279, 729, 13, 715, 2324, 37835, 25, 220, 16, 15, 15, 27, 11159, 17638, 29, 5113, 62, 16, 25, 22555, 11, 902, 3491, 13, 358, 686, 387, 2952, 311, 1492, 13, 3017, 4226, 374, 25]\n"
     ]
    }
   ],
   "source": [
    "dummy = 'def dummy(): \\n    \"\"\"\\n    \"\"\"\\n pass'\n",
    "dummy_prompt = prompt_fmt_ruby(dummy)\n",
    "few_shot_toks = len(tokenizer.encode(\n",
    "    dummy_prompt)) - len(tokenizer.encode(dummy))\n",
    "print(f\"Few-shot prompt has {few_shot_toks} tokens\")\n",
    "print(tokenizer.encode(\n",
    "    dummy_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a4e6e-e67a-4207-9c74-9e2674946387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for ex in tqdm(dataset, total=len(dataset), desc=\"Generating prompts\"):\n",
    "    code = ex[\"content\"]\n",
    "    oks = len(tokenizer.encode(code)) + few_shot_toks\n",
    "    if toks > 16380:\n",
    "        print(f\"Skipping example with {toks} tokens\")\n",
    "        # to skip, just add dummy prompt\n",
    "        prompts.append(dummy_prompt)\n",
    "        continue\n",
    "    p = prompt_fmt_ruby(code)\n",
    "    prompts.append(p)\n",
    "\n",
    "responses = []\n",
    "for chunk in tqdm(chunkify(prompts, 512), desc=\"Generating responses\"):\n",
    "    outs = model.generate(chunk, SamplingParams(\n",
    "        temperature=0.0, stop=\"\\n\", max_tokens=5))\n",
    "    contents = [o.outputs[0].text for o in outs]\n",
    "    for c in contents:\n",
    "        yes_count = c.lower().count(\"yes\")\n",
    "        no_count = c.lower().count(\"no\")\n",
    "        if yes_count > no_count:\n",
    "            responses.append(True)\n",
    "        elif yes_count < no_count:\n",
    "            responses.append(False)\n",
    "        else:\n",
    "            # default to No\n",
    "            responses.append(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e6021-71c7-449b-a54a-d6f9a54e4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02eef9-f557-4f36-adf3-0115a5c08041",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5579e1-7c33-438e-8e79-d8acab10b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in prompts:\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da440e82-7104-4c01-99dc-b7dac8803cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = subset.filter(  # horrible hack!\n",
    "    #lambda ex, i: responses[i] and \"def dummy()\" not in ex[\"content\"], with_indices=True)\n",
    "    lambda ex, i: prompts[i] and \"def dummy()\" not in ex[\"content\"], with_indices=True)\n",
    "print(f\"Filtered {len(dataset) - len(new_ds)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85717aaa-75e0-4396-addd-529302cc271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in new_ds:\n",
    "    print(ex['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e497d-4afe-4651-b73c-5950e57b06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.save_to_disk(\"./datasets/seed3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148b29c-1dc5-47a3-bef7-60dc19d75764",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
